# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

---

## TRAINING LOGGING RULES (–ö–†–ò–¢–ò–ß–ù–û!)

–ü—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –í–°–ï–ì–î–ê –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤ –ª–æ–≥–∞—Ö:

1. **–í –Ω–∞—á–∞–ª–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏:**
   - GPU name (nvidia-smi)
   - VRAM total / used
   - batch_size
   - Estimated steps/sec (–ø–æ—Å–ª–µ –ø–µ—Ä–≤—ã—Ö 100 steps)

2. **–ö–∞–∂–¥—ã–µ N steps (eval):**
   - Current step / total steps
   - val_nll (–∏ best)
   - Elapsed time
   - **steps/sec** (—Ç–µ–∫—É—â–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å!)
   - **ETA** (—Å–∫–æ–ª—å–∫–æ –æ—Å—Ç–∞–ª–æ—Å—å!)
   - patience (–µ—Å–ª–∏ early-stop)

3. **–í –∫–æ–Ω—Ü–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏:**
   - Total time
   - Total steps
   - Final steps/sec
   - samples/sec
   - Cost estimate ($/hr √ó time)

**–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ –ª–æ–≥–∞:**
```
Step 1000/20000 | val_nll=0.358 (best=0.358) | 5.8 steps/s | ETA: 55m | elapsed: 2.9m
```

**–ó–ê–ß–ï–ú:** –ß—Ç–æ–±—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å GPU –∏ –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ–¥!
–°–º. `docs/runpod_specs.md` –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤.

---

## DOCUMENTATION RULES (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!)

### –ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –æ–±–Ω–æ–≤–ª—è–π:

1. **docs/PROJECT_MAP.md** ‚Äî –≥–ª–∞–≤–Ω–∞—è –∫–∞—Ä—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞
   - Experiments Timeline (–Ω–æ–≤—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã)
   - File Map (–Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã)
   - Implementation Checklist (—á—Ç–æ —Å–¥–µ–ª–∞–Ω–æ)
   - Key Insights Log (–≤–∞–∂–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç–∏—è)

2. **e*_summary.md** ‚Äî —Å–∞–º–º–∞—Ä–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (e3_summary.md, e4_summary.md –∏ —Ç.–¥.)
   - –°–æ–∑–¥–∞–≤–∞—Ç—å –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –Ω–æ–≤–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
   - –û–±–Ω–æ–≤–ª—è—Ç—å –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –ß—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å:
- –ù–æ–≤—ã–µ training —Å–∫—Ä–∏–ø—Ç—ã
- –ò–∑–º–µ–Ω–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ (NLL, –º–µ—Ç—Ä–∏–∫–∏)
- –ë–∞–≥–∏ –∏ –∏—Ö —Ä–µ—à–µ–Ω–∏—è
- –ò–Ω—Å–∞–π—Ç—ã –∏ –≤—ã–≤–æ–¥—ã

### –§–æ—Ä–º–∞—Ç Key Insights:
```markdown
### YYYY-MM-DD: –ö—Ä–∞—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
- **Problem/Observation:** —á—Ç–æ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏
- **Evidence:** –¥–∞–Ω–Ω—ã–µ/–º–µ—Ç—Ä–∏–∫–∏
- **Conclusion/Action:** —á—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ
```

---

## Project Overview

**nanoGPT_RH** ‚Äî Neural telescope for Riemann Hypothesis spectral analysis. We train a small transformer (nanoGPT) on 2M unfolded zeta zeros to:
1. Learn stationary statistics (GUE-like spacing distribution, spectral rigidity)
2. Extract hidden state geometry ("helmet" manifold)
3. Distill operator/kernel approximation via attention logits + PySR symbolic regression

This is NOT a "prove RH with neural nets" project. It's a controlled lab to study spectral invariants and compare with Q3 kernel/operator structures (Toeplitz-RKHS bridge, prime cap, uniform floor c*=11/10).

## Architecture Philosophy

- **nanoGPT over large LLMs** ‚Äî we need observability, not chat power
- **Continuous or binned spacings** ‚Äî NOT text tokenization (avoid "14.1347" as chunks)
- **Tabula rasa** ‚Äî if small model learns spectral invariants from scratch, that's stronger scientific signal
- **Attention ‚âà Kernel** ‚Äî attention logits A_ij as function of distance d=|u_i-u_j|

## Data Pipeline

### Unfolding (critical preprocessing)

Raw zeros ‚Üí unfolded spacings with mean ‚âà 1:

**Variant A (local density):**
```
Œî_n = Œ≥_{n+1} - Œ≥_n
s_n = Œî_n * log(Œ≥_n) / (2œÄ)
```

**Variant B (unfolded coordinates):**
```
u(Œ≥) = (Œ≥/2œÄ) * log(Œ≥/(2œÄe))
s_n = u(Œ≥_{n+1}) - u(Œ≥_n)
```

Quality check: mean(s) ‚âà 1 on large blocks.

### Sequence formatting
- Sequence length L=256 (configurable)
- Train/val split BY BLOCKS (no shuffling ‚Äî preserves structure)

## Commands

```bash
# Setup environment (uv + Python 3.13)
uv venv && source .venv/bin/activate
uv pip install torch numpy scipy matplotlib rich

# Data prep
python scripts/prepare_continuous_2M.py --input zeros_2M.txt --output data/continuous_2M

# E4 Training (POSTFIX + ID-Detox)
python src/train_mdn_postfix.py \
  --data-dir data/continuous_2M \
  --out-dir out/mdn_postfix_E4_s7 \
  --seed 7 \
  --slot-id-mode permute_per_batch \
  --use-aux-loss \
  --early-stop --patience 800 \
  --batch-size 512 --use-amp

# Evaluation
python src/eval_mdn.py --ckpt checkpoints/E4_s7_best.pt --data-dir data/continuous_2M

# Diagnostics
python src/diagnose_memory_postfix.py \
  --ckpt checkpoints/E4_s7_best.pt \
  --data-dir data/continuous_2M \
  --output-dir results/E4_s7
```

## Key Experiments

### Baselines (mandatory sanity checks)
1. **Shuffled spacings** ‚Äî destroy correlations, keep marginals
2. **i.i.d. resample** ‚Äî sample from empirical distribution
3. **Positional encoding only** ‚Äî no learned weights

### Metrics
- Spacing histogram vs GUE Wigner surmise: P(s) = (œÄs/2)exp(-œÄs¬≤/4)
- Spectral form factor: ramp ‚Üí plateau transition
- Hidden state manifold stability across seeds

### Kernel extraction
1. Collect attention logits A_ij before softmax
2. Build dataset (d_k, y_k) where d = |u_i - u_j|
3. Run PySR: look for sine-kernel-like forms on Pareto front

## Model Choice

**Phase 1:** nanoGPT / minGPT / tiny Transformer (fast iterations on Mac)
**Phase 2:** Larger models when metrics and signal are clear

## Loss Design

- **Primary:** next-spacing prediction (MSE for regression, CE for bins)
- **Diagnostics only (not in loss):** Mehta/GUE distribution ‚Äî use as external validator, not built-in (avoids "you forced the network" criticism)
- **Optional soft regularizers:** penalty for too many tiny spacings (level repulsion)

## Repository Structure (Jan 2026)

```
nanoGpt_RH/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                      # MAIN CODE
‚îÇ   ‚îú‚îÄ‚îÄ train_mdn.py             # Base SpacingMDN (MDNConfig, transformer)
‚îÇ   ‚îú‚îÄ‚îÄ train_mdn_postfix.py     # E4 POSTFIX training (ID-Detox, aux-loss)
‚îÇ   ‚îú‚îÄ‚îÄ train_mdn_memory.py      # PREFIX memory (deprecated)
‚îÇ   ‚îú‚îÄ‚îÄ eval_mdn.py              # Evaluation (NLL, CRPS, PIT)
‚îÇ   ‚îú‚îÄ‚îÄ diagnose_memory.py       # PREFIX diagnostics
‚îÇ   ‚îî‚îÄ‚îÄ diagnose_memory_postfix.py # POSTFIX diagnostics (A-K metrics)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/                  # UTILITIES
‚îÇ   ‚îú‚îÄ‚îÄ prepare_continuous_2M.py # Unfolding zeros ‚Üí spacings
‚îÇ   ‚îú‚îÄ‚îÄ prepare_zeros.py         # Raw zeros processing
‚îÇ   ‚îú‚îÄ‚îÄ prepare_primes.py        # Prime gaps dataset
‚îÇ   ‚îî‚îÄ‚îÄ runpod_setup.sh          # RunPod setup script
‚îÇ
‚îú‚îÄ‚îÄ üìÅ checkpoints/              # TRAINED MODELS (Git LFS)
‚îÇ   ‚îú‚îÄ‚îÄ E0_baseline_best.pt      # SpacingMDN no memory
‚îÇ   ‚îú‚îÄ‚îÄ E1_prefix_best.pt        # PREFIX memory (decorative)
‚îÇ   ‚îú‚îÄ‚îÄ E2_prefix_best.pt        # PREFIX memory (seed variance)
‚îÇ   ‚îú‚îÄ‚îÄ E3_postfix_s1337_best.pt # POSTFIX (ID-crutch)
‚îÇ   ‚îú‚îÄ‚îÄ E4_s7_best.pt            # ‚≠ê BEST! NLL=0.1942, ID-Detox works!
‚îÇ   ‚îî‚îÄ‚îÄ E4_s1337_best.pt         # POSTFIX + ID-Detox (stuck seed)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                     # DATASET
‚îÇ   ‚îú‚îÄ‚îÄ continuous_2M/           # Main dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.pt             # (7035, 256) training spacings
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ val.pt               # (781, 256) validation spacings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ meta.pt              # Dataset metadata
‚îÇ   ‚îú‚îÄ‚îÄ train.pt, val.pt         # Copies in root
‚îÇ   ‚îî‚îÄ‚îÄ *_primes.pt              # Prime gaps dataset
‚îÇ
‚îú‚îÄ‚îÄ üìÅ docs/                     # DOCUMENTATION
‚îÇ   ‚îú‚îÄ‚îÄ PROJECT_MAP.md           # ‚≠ê MAIN PROJECT MAP (experiments, results)
‚îÇ   ‚îú‚îÄ‚îÄ E4_SPEC.md               # E4 specification (ID-Detox)
‚îÇ   ‚îú‚îÄ‚îÄ runpod_specs.md          # GPU comparison & benchmarks
‚îÇ   ‚îú‚îÄ‚îÄ PROJECT_GUIDE.md         # Detailed project guide
‚îÇ   ‚îî‚îÄ‚îÄ *.md                     # Session summaries, drafts
‚îÇ
‚îú‚îÄ‚îÄ üìÅ results/                  # DIAGNOSTICS OUTPUT
‚îÇ   ‚îî‚îÄ‚îÄ E4_s7/
‚îÇ       ‚îú‚îÄ‚îÄ postfix_diagnostics.jsonl  # Metrics JSON
‚îÇ       ‚îî‚îÄ‚îÄ postfix_diagnostics.png    # Visualization
‚îÇ
‚îú‚îÄ‚îÄ üìÅ archive/                  # OLD CODE (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ old_training/            # Deprecated train_*.py
‚îÇ   ‚îú‚îÄ‚îÄ analysis/                # Old analysis scripts
‚îÇ   ‚îî‚îÄ‚îÄ images/                  # Old PNG files
‚îÇ
‚îú‚îÄ‚îÄ CLAUDE.md                    # THIS FILE
‚îú‚îÄ‚îÄ README.md                    # Project readme
‚îî‚îÄ‚îÄ .gitignore                   # Git ignore rules
```

### Current Status: E4 COMPLETE ‚úÖ
- **Best Model:** `checkpoints/E4_s7_best.pt`
- **NLL:** 0.1942 (+36% vs E3!)
- **ID-Detox:** Works! Perm Inc = 1.0%
- **Next:** E5 (slot specialization) or symbolic extraction

## Q3 Integration Points

Cross-reference with Q3 formal structures:
- Attention logits ‚Üí compare with sine kernel / Toeplitz symbol
- Operator norm cap ‚Üí check if learned interactions respect bounds
- Prime block structure ‚Üí compare attention patterns with œÅ(t) formulation

## Notes

- 256 bins for spacing classification gives stable perplexity metric
- RoPE / sinusoidal positional encoding justified for sequence + phase structure
- Save hidden states during training for manifold analysis

## RunPod GPU Training Guide

### –û–î–ù–ê –ö–û–ú–ê–ù–î–ê –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ pipeline

**–ù–ê –ú–ê–ö–ï (–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞):**
```bash
cd /Users/emalam/Documents/GitHub/nanoGpt_RH

# –°–æ–∑–¥–∞—Ç—å –ø–∞–∫–µ—Ç
tar czf runpod_package.tar.gz \
  src/train_mdn.py src/train_mdn_postfix.py \
  src/eval_mdn.py src/diagnose_memory_postfix.py \
  scripts/runpod_setup.sh data/continuous_2M

# –û—Ç–ø—Ä–∞–≤–∏—Ç—å (–ø–æ–ª—É—á–∏—à—å –∫–æ–¥ —Ç–∏–ø–∞: 2406-final-rufus-fashion-5)
runpodctl send runpod_package.tar.gz
```

**–ù–ê –ü–û–î–ï (–æ–¥–Ω–∞ –∫–æ–º–∞–Ω–¥–∞!):**
```bash
cd /workspace && runpodctl receive <–ö–û–î> && tar xzf runpod_package.tar.gz && chmod +x runpod_setup.sh && ./runpod_setup.sh
```

**–°–ö–ê–ß–ê–¢–¨ –†–ï–ó–£–õ–¨–¢–ê–¢–´:**
```bash
# –ù–∞ –ø–æ–¥–µ:
tar czf results.tar.gz out/ reports/ && runpodctl send results.tar.gz

# –ù–∞ –º–∞–∫–µ:
runpodctl receive <–ö–û–î>
tar xzf results.tar.gz
```

### –ü—Ä–æ–±–ª–µ–º—ã —Å Web Terminal / Jupyter

**–ò–ó–í–ï–°–¢–ù–´–ô –ë–ê–ì RUNPOD!** –í–µ–±-—Ç–µ—Ä–º–∏–Ω–∞–ª –∏ Jupyter —á–∞—Å—Ç–æ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç:
- –ü–æ—Å–ª–µ —Ä–µ—Å—Ç–∞—Ä—Ç–∞ –ø–æ–¥–∞ –≤–µ–±-—Ç–µ—Ä–º–∏–Ω–∞–ª –º–æ–∂–µ—Ç –Ω–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å—Å—è
- Jupyter –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç 502 Bad Gateway
- –¢–µ—Ä–º–∏–Ω–∞–ª –≤ Jupyter –ø—É—Å—Ç–æ–π/–Ω–µ –ø–µ—á–∞—Ç–∞–µ—Ç

**–†–ï–®–ï–ù–ò–Ø:**
1. **–ò—Å–ø–æ–ª—å–∑—É–π SSH –≤–º–µ—Å—Ç–æ –≤–µ–±-—Ç–µ—Ä–º–∏–Ω–∞–ª–∞** (–Ω–∞–¥–µ–∂–Ω–µ–µ):
   ```bash
   # Proxy SSH (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ–≥–¥–∞, –Ω–æ –±–µ–∑ SCP):
   ssh espzoobm5yxuif-64410f63@ssh.runpod.io -i ~/.ssh/id_ed25519

   # –ò–ª–∏ Direct TCP (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω):
   ssh -p <PORT> root@<IP> -i ~/.ssh/id_ed25519
   ```

2. **–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏ –ø–æ–¥** - –∏–Ω–æ–≥–¥–∞ –≤–µ–±-—Ç–µ—Ä–º–∏–Ω–∞–ª —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ

3. **–ü—Ä–æ–≤–µ—Ä—å —à–∞–±–ª–æ–Ω –ø–æ–¥–∞** - —Ç–æ–ª—å–∫–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã RunPod –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç Jupyter

4. **–ü–æ–¥–æ–∂–¥–∏ 2-3 –º–∏–Ω—É—Ç—ã** –ø–æ—Å–ª–µ —Å—Ç–∞—Ä—Ç–∞ –ø–æ–¥–∞ - –∏–Ω–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –≤—Ä–µ–º—è

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ runpodctl (–æ–¥–∏–Ω —Ä–∞–∑)
```bash
brew install runpod/runpodctl/runpodctl
```

### SSH Access (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
–î–ª—è Direct TCP SSH, –¥–æ–±–∞–≤—å –∫–ª—é—á –ù–ê –ü–û–î–ï:
```bash
mkdir -p ~/.ssh && chmod 700 ~/.ssh
echo "ssh-ed25519 AAAA... —Ç–≤–æ–π_–∫–ª—é—á" >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
```

### GPU Selection (Jan 2026)

**DEFAULT CHOICE: L40S @ $0.86/hr**
- 48GB VRAM, Ada architecture, ML-optimized
- High availability, datacenter drivers
- Best balance of price/performance/availability

**BUDGET: A40 @ $0.40/hr**
- 48GB VRAM, Ampere (older but cheap)
- Best $/performance for single jobs

**PARALLEL SEEDS: RTX 6000 Ada @ $0.77/hr**
- 48GB VRAM, can run 3 seeds @ batch=512

See `docs/runpod_specs.md` for full GPU comparison.

### Performance Tips
- **L40S 48GB**: batch_size 512-1024, --use-amp
- **H100 80GB**: batch_size 2048, --use-amp --compile
- **A40 48GB**: batch_size 512, --use-amp
- FlashAttention –≤–∫–ª—é—á–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ train_mdn.py

### –ß—Ç–æ –¥–µ–ª–∞–µ—Ç runpod_setup.sh
1. pip install torch numpy scipy matplotlib rich
2. –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
3. train_mdn.py (20k steps, ~50 –º–∏–Ω –Ω–∞ H100)
4. eval_mdn.py (–º–µ—Ç—Ä–∏–∫–∏)
5. train_mdn_memory.py (10k steps, ~30 –º–∏–Ω)
6. diagnose_memory.py (–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞)

## GPU Benchmark (2024-12-31)

### Speed Comparison (SpacingMDN+Memory, 4.8M params)

| GPU | $/hr | steps/s | batch | Œî per 500 steps | samples/s |
|-----|------|---------|-------|-----------------|-----------|
| Mac M4 Max | FREE | 1.3 | 128 | ~3.2m | 166 |
| A40 48GB | $0.40 | 5.8 | 512 | ~1.4m | 2,969 |
| H100 80GB | $3.69 | ~15* | 512 | ~33s* | ~7,680* |

*H100 estimated based on typical 2.5x speedup over A40

### Cost Analysis (20k steps)

| GPU | Time | Cost | Cost/10k steps |
|-----|------|------|----------------|
| Mac M4 Max | ~4.3 hr | $0 | $0 |
| A40 | ~57 min | ~$0.38 | $0.19 |
| H100 | ~22 min | ~$1.35 | $0.68 |

### Recommendation

**A40 = –ª—É—á—à–∏–π –≤—ã–±–æ—Ä –ø–æ —Ü–µ–Ω–∞/–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –Ω–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π!**
- 18x –±—ã—Å—Ç—Ä–µ–µ Mac –ø–æ samples/sec
- 3.5x –¥–µ—à–µ–≤–ª–µ H100 –ø—Ä–∏ 2.5x –º–µ–Ω—å—à–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
- 48GB VRAM –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è batch_size=512

–î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π (<10M params) H100 –∏–∑–±—ã—Ç–æ—á–Ω–∞.
