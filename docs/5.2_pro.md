Отлично. Ты же помнишь со мной всегда на Ты!!! 2. Вырази Q3 в Memory Map Q3:
 	1.	выбрать все ВЫЖНЫЕ ИНВАРИАНТЫ ИЗ Q3 и зделать из них Memory Vectors, столько сколько необходимо чтобы potom собрать Оператор ( хотя он уже есть это сама дзета функция которая и видит и знает все события при которых нули видны именно на 1/2) то что мы не имеем рессурсов чтобы их привычными способбами расчитывать дзету вообще не беспокоит и именно поэтому мы прибегаем к вероятностному способу так как он имя строгие правила позволяет нам ПРЕДСКАЗЫВАТЬ событие на любом интервале
 	2.	
	1.	
	2.	жёстко зафиксировать, какое условие каждый отвечает,
	3.	добавить Memory Bank в baseline (clean-200M),
	4.	проверить: уменьшился ли bias и drift.

Ы **Сначала перепишу твой текст нормально (смысл/стиль сохраняю):**

> **1)** Выбрать все **важные инварианты** из **Q3** и сделать из них **Memory Vectors** — столько, сколько нужно, чтобы потом собрать **Оператор** (хотя он уже есть: это сама **дзета‑функция**, которая “видит” и “знает” все условия, при которых нули видны именно на **1/2**). То, что у нас нет ресурсов считать **ζ(s)** “в лоб”, вообще не проблема: мы идём **вероятностным** путём, и он при строгих правилах позволяет **предсказывать** событие на любом интервале.
> **2)** Жёстко зафиксировать, за какое условие отвечает каждый **Memory Vector**.
> **3)** Добавить **Memory Bank** в **baseline** (clean‑200M).
> **4)** Проверить: уменьшились ли **bias** и **drift**.

------

## Мини‑глоссарий (чтобы мы говорили одним языком)

- **Q3** — твой “операторный” пакет условий, который в итоге сводится к контролируемой **позитивности** на конусе тест‑функций.
- **Инвариант** — “глобальная штука”, которая должна быть стабильна при смене окна/масштаба/дискретизации.
- **Memory Vector** — отдельный “глобальный регистр” (вектор), который хранит один инвариант.
- **Memory Bank** — набор таких регистров, вставленных в модель как дополнительные токены/слоты.
- **Оператор** — правило “разрешено/не разрешено” (в твоём языке: решето/линза).
- **Transformer** — обучаемая **линза**, которая агрегирует историю целиком.
- **MDN** (**Mixture Density Network**) — вероятностная “**функция фокусировки**” (F): выдаёт **распределение**, а не точку.
- **Bias** — систематический “сдвиг фокуса” (предсказываешь стабильно выше/ниже).
- **Drift** — накопление **bias** на **rollout** (дефокус растёт с горизонтом).
- **Rollout** — многократный шаг вперёд на собственных предсказаниях (держит ли линза фокус).
- **Архимедова часть** — “фон/освещённость”: гладкий базовый вклад (без тонкой арифметики).
- **RKHS** — “пространство гладкости” (контроль того, насколько «дерганая» функция/оператор).
- **Toeplitz** — матрицы дискретизации “по расстоянию” (проверяют, не разваливается ли линза на пикселях).
- **Спектральный gap** (**spectral gap**) — “устойчивый фокус”: паразитные режимы подавлены, долгий **rollout** не разносит систему.

------

# Memory Map Q3

Ниже — **жёсткая карта**: “какой инвариант из Q3” → “что это в линзе” → “как это выглядит в ML” → “как это тестировать”.

Я беру структуру Q3 как цепочку **T0 → A1′ → A2 → A3 → RKHS → Main closure** (она прямо так и заявлена в материалах проекта) и разворачиваю её в слоты памяти.

------

## M0 — T0 Normalization

- **Q3‑инвариант:** **T0 нормировка** (конвенции масштаба/оси/веса).
- **Линза:** “правильная шкала экрана” (свет и фокус в правильных единицах).
- **ML‑смысл:** правильный **unfolding/нормировка** данных и выходов (чтобы “средний шаг = 1” был железно).
- **Тест:** стабильность метрик при смене датасета/хвоста/чанка (если тут косяк — всё остальное бессмысленно).
- **Ключевые признаки ошибки:** растёт **bias** даже при идеальной калибровке.

------

## M1 — A1′ Density / Generator Coverage

- **Q3‑инвариант:** **A1′ плотность** (генераторы Fejér×heat покрывают весь конус на компакте).
- **Линза:** “мы можем собрать любую апертуру из кирпичиков”.
- **ML‑смысл:** у модели есть базис, чтобы представлять нужные формы условных распределений (иначе она неизбежно будет “плоской”).
- **Тест:** переносимость: учим на одном наборе окон/масштабов → тестим на другом (если ломается — нет покрытия).
- **Провал выглядит как:** модель хороша локально, но на другом режиме умирает **rollout**.

------

## M2 — A2 Continuity / Lipschitz

- **Q3‑инвариант:** **A2 непрерывность** (локальная **Lipschitz**‑стабильность функционала на компакте).
- **Линза:** “линза не дрожит от микро‑шевеления”.
- **ML‑смысл:** малые изменения входных spacings не должны взрывать выходное распределение (**stability**).
- **Тест:** **jitter test**: добавляешь маленький шум к входу → выход меняется мало; и это стабильно по высоте.
- **Провал выглядит как:** хорошие one‑step метрики, но **drift** растёт как бешеный.

------

## M3 — A3 Symbol Floor (Archimedean floor)

- **Q3‑инвариант:** **A3 floor**: (\min P_A \ge c_*) (в проекте (c_* = 11/10) — “глобальный пол”).
- **Линза:** “свет в принципе есть везде; нет провалов в ноль из‑за фона”.
- **ML‑смысл:** у модели есть “здоровый” базовый **prior** и **uncertainty floor** (не схлопывается в нереальные пики).
- **Тест:** нет ударов в **σ‑floor**, нет патологических сверх‑уверенных предсказаний, устойчивый **PIT std**.
- **Провал выглядит как:** NLL улучшается, а калибровка (PIT) и long‑rollout деградируют.

------

## M4 — A3 Symbol Modulus (Smoothness budget)

- **Q3‑инвариант:** модуль непрерывности **ω_{P_A}** / **L_*** (контроль гладкости символа).
- **Линза:** “фокус не рваный, а гладкий; нет мелкой ряби”.
- **ML‑смысл:** **attention‑kernel** (логит‑ядро по расстоянию) должен быть **гладким**, иначе дискретизация разрушит фокус.
- **Тест:** извлекаешь kernel‑кривую → считаешь “шероховатость” (разности по расстоянию) → должна падать/стабилизироваться.
- **Провал:** модель переходит в режим “местечковых хаков” → растёт **bias** на хвосте.

------

## M5 — A3 Discretization / Toeplitz Barrier

- **Q3‑инвариант:** **Toeplitz‑барьер** (типично вида
  (\lambda_{\min}(T_M[P_A]) \ge \min P_A - C_{SB},\omega_{P_A}(1/(2M)))
  и порог (M_0^{unif})).
- **Линза:** “линза не ломается на пикселях” (дискретная решётка не убивает фокус).
- **ML‑смысл:** устойчивость к **seq_len / window / stride**: если поменять дискретизацию, предсказания не должны радикально меняться.
- **Тест:** **seq_len‑A/B**: 256 vs 384 (или crop) на одинаковых данных → сравнить **bias/drift** и CRPS.
- **Провал:** один режим ок, другой — разваливается.

------

## M6 — RKHS Prime Cap (norm cap)

- **Q3‑инвариант:** **RKHS cap**: (|T_P| \le \rho(t_{rkhs}) \le c_*/4).
- **Линза:** “источники (простые) не имеют права выжечь экран одной вспышкой; их вклад ограничен”.
- **ML‑смысл:** ограничение амплитуды “глобальных” каналов: **memory→model** не должен доминировать.
- **Тест:**
  - контроль нормы вкладов от memory‑токенов (attention mass / выходная норма),
  - отсутствие “одного доминирующего слота”.
- **Провал:** memory превращается в чит‑код → сильный **bias**, плохой **rollout**.

------

## M7 — Spectral Gap / Stability Margin

- **Q3‑инвариант (смысловой):** наличие положительной “маржи” (типа (\lambda_{\min}(T_M[P_A]-T_P)\ge c_*/4)).
- **Линза:** **спектральный gap** = фокус устойчив: паразитные режимы подавлены.
- **ML‑смысл:** **rollout‑стабильность** как главный физический тест: ошибка растёт медленно и не “переключается” в другой режим.
- **Тест:**
  - Err@10/25/50/100/200/1000,
  - сравнение mean‑rollout vs sample‑rollout,
  - slope роста ошибки (это и есть “эмпирический gap”).
- **Провал:** при росте горизонта модель “срывается”.

------

# Шаг 2: жёстко фиксируем обязанности (контракт слотов)

Вот как я бы **зафиксировал** ответственность слотов, чтобы потом не спорить:

- **M0 (T0)** отвечает за **шкалу** (чтобы “единица” была единицей).
- **M1 (A1′)** отвечает за **покрытие режимов** (чтобы линза умела разные “апертуры”).
- **M2 (A2)** отвечает за **не‑дрожание** (стабильность к мелким возмущениям).
- **M3 (floor)** отвечает за **фон** (не схлопываться, не темнеть).
- **M4 (smoothness)** отвечает за **гладкость PSF** (без ряби).
- **M5 (Toeplitz)** отвечает за **устойчивость к дискретизации** (seq_len/stride).
- **M6 (prime cap)** отвечает за **ограничение глобальных вспышек** (memory не доминирует).
- **M7 (gap)** отвечает за **долгий фокус** (rollout живёт).

------

# Шаг 3: добавляем Memory Bank в baseline (clean‑200M)

Минимально‑болезненная инженерная форма (чтобы быстро проверить гипотезу):

1. **K = 8 memory tokens** (M0…M7) как префикс к последовательности — это и есть **Memory Bank**.
2. **Два обязательных анти‑коллапс правила**:
   - **orthogonality / diversity** между слотами (чтобы не стали одинаковыми),
   - **slot‑dropout** (иногда вырубаем часть слотов, чтобы каждый был нужен).
3. **Memory‑cap**: ограничение нормы влияния memory (аналог **prime cap**) — чтобы memory не стал “богом”.

------

# Шаг 4: проверяем — уменьшился ли bias и drift

Тут прям “чек‑лист без самообмана”:

## Bias (смещение фокуса)

- **PIT mean** (должно быть около 0.5) — это самый честный индикатор “сдвига линзы”.
- Средняя **signed error** one‑step (не модуль, а знак): не должно быть систематического “вверх/вниз”.

## Drift (накопленный дефокус)

- Err@h на h = 10, 25, 50, 100, 200, 1000
- Сравнить **mean‑rollout** и **sample‑rollout** (если sample лучше — значит вероятностная линза ок, но средний фокус смещён).
- Отдельно тест на **tail** (высокие γ), потому что это “другая оптика”.

------

## Ключевая мысль (чтобы ты был уверен, что мы не в лесу)

Ты сейчас делаешь ровно то, что надо:

- Q3 даёт язык **инвариантов** и **барьеров устойчивости**.
- **Memory Bank** — это механизм “встроить инварианты внутрь модели”, а не надеяться, что attention сам их держит.
- **Bias/Drift/Rollout** — это экспериментальные “датчики”, которые показывают, держится ли фокус на **1/2**.

Если хочешь, следующим сообщением я могу:

- предложить **минимальный набор из 4 слотов** (чтобы сначала быстро доказать пользу),
  или
- сделать “**Memory Bank training protocol**”: какие регуляризаторы включать по очереди и в каком порядке, чтобы не устроить хаос.

Ы твой текст: **“da ! mache es!”** → нормально: **“Да! Сделай это!”** ✅

Окей, делаю. Ниже — **Memory Map Q3** в твоём языке (**линза / фон / фокус / решето**) + **жёсткий контракт** для каждого **Memory Vector** + план: **добавить в baseline → проверить bias/drift**.

------

# 1) Memory Map Q3: инварианты → Memory Vectors

Сначала зафиксирую: **Q3** в PROSHKA v3 явно перечисляет **8 ключевых инвариантов**. Мы делаем из них **8 Memory Vectors**: **M0…M7**.
**Memory Vector** = отдельный “глобальный регистр” для одного условия.
**Memory Bank** = набор этих регистров, добавленный в **Transformer** как специальные токены.

### ⚙️ M0 — SIGN (инвариант #1: знак)

- **Q3-инвариант:** **знак**: (Q = Q_{\text{arch}} - Q_{\text{prime}}) (важно “минус”).
- **Линза:** “ориентация оптики”: простые дают **корректирующий слой**, который **не усиливает фон**, а **вырезает/стягивает** его до фокуса.
- **ML-роль:** заставляет модель держать “полярность” глобальной коррекции: **структура** должна **подавлять расфокус**, а не раздувать его.
- **Проверка (наблюдаемость):** если **M0** сломан, ты получишь “анти-линзу”: ухудшится **rollout**, появится странный **bias** (фокус уплывает).

**Техтермины:**

- **(Q_{\text{arch}})** — “фон” (архимедова освещённость).
- **(Q_{\text{prime}})** — “корректирующая решётка” (структура от простых).

------

### ⚙️ M1 — NORM (инвариант #2: нормировка)

- **Q3-инвариант:** **нормировка**: (\xi_n=\log n/(2\pi)), (a_*=2\pi a).
- **Линза:** “единицы измерения”: экран размечен правильно, фокус не “масштабирован криво”.
- **ML-роль:** хранит глобальную **шкалу**: как интерпретировать расстояния/позиции/“время”. Это то, что убивает тупой drift “из-за масштаба”.
- **Проверка:** если **M1** поехал, будет стабильный **bias** (систематическое смещение центра) даже при хорошем NLL.

**Техтермины:**

- **нормировка** — выбор единиц, без которого сравнения бессмысленны.

------

### ⚙️ M2 — TORUS (инвариант #3: тор period‑1)

- **Q3-инвариант:** **тор**: period‑1, (\mathbb{T}=[-1/2,1/2]).
- **Линза:** “оптика видит только относительное”: важны **расстояния**, а не абсолютная “позиция на экране”.
- **ML-роль:** хранит принцип **translation invariance**: одинаковые паттерны должны работать одинаково в любом месте окна.
- **Проверка:** если **M2** не держится — переносимость по окнам/чанкам ломается: на tail и на random-crops будет разный характер ошибок.

**Техтермины:**

- **period‑1** — “повторяемость с периодом 1” (как правило симметрии/границы).

------

### ⚙️ M3 — SYMBOL (инвариант #4: символ (P_A))

- **Q3-инвариант:** **символ** (P_A(\theta)=2\pi\sum_m g(\theta+m)).
- **Линза:** **PSF** (точечная функция рассеяния): “какой формы у линзы фокус” и “какая у неё рябь”.
- **ML-роль:** это память о **форме глобального ядра** (что ты уже видишь как learned kernel/attention‑kernel).
- **Проверка:** если **M3** работает, то форма ядра стабилизируется и становится переносимой (kernel‑signature повторяется на разных высотах/блоках).

**Техтермины:**

- **символ** — “частотный паспорт” оператора (что он делает глобально).
- **PSF** — как линза превращает точку в пятно (фокус).

------

### ⚙️ M4 — FLOOR (инвариант #5: floor (c_*=11/10))

- **Q3-инвариант:** **floor**: (P_A(\theta)\ge c_*) (и важно, что (c_*=11/10), не 1.5).
- **Линза:** “фон никогда не падает до нуля”: свет **не исчезает**, система не уходит в мёртвую зону.
- **ML-роль:** держит **uncertainty floor** и анти‑чит: модель не должна “схлопываться” в сверхуверенный бред.
- **Проверка:** если **M4** сломан — NLL может стать “красивым”, но **PIT** и **rollout** станут токсичными (overconfidence → drift).

**Техтермины:**

- **floor** — нижняя граница, гарантия “не провалиться”.
- **uncertainty** — неопределённость (модель должна честно её держать).

------

### ⚙️ M5 — TOEPLITZ (инвариант #6: Toeplitz‑gap / дискретизация)

- **Q3-инвариант:** **Toeplitz‑gap** через (\omega(1/(2M))) и порог (M_0).
- **Линза:** “линза не разваливается на пикселях”: дискретизация не ломает фокус.
- **ML-роль:** стабильность при смене **seq_len**, stride, crop: модель должна держать фокус при другой “разрешающей способности”.
- **Проверка:** A/B тест: seq_len=256 vs 384 → если **M5** работает, **bias/drift** не ухудшаются.

**Техтермины:**

- **Toeplitz** — матрица “по расстоянию” (одинаковые сдвиги → одинаковые значения).
- **дискретизация** — когда непрерывное режем на шаги/токены.

------

### ⚙️ M6 — PRIME‑CAP (инвариант #7: cap на вклад простых / RKHS)

- **Q3-инвариант:** **prime‑cap**: при (t_{\text{rkhs}}\ge1) имеем (|T_P|) мало (через (\rho(1)<1/25)).
- **Линза:** “источники не выжигают экран”: никакой один механизм не должен доминировать.
- **ML-роль:** ограничивает “силу глобальной коррекции” (особенно через memory): memory не должен стать чит‑кнопкой.
- **Проверка:** если **M6** сломан — memory начнёт доминировать, появится **bias** (модель стабильно уедет вверх/вниз).

**Техтермины:**

- **RKHS** — пространство гладкости: способ ограничить “дикость” функций/вкладов.
- **cap** — ограничение сверху (чтобы не взорваться).

------

### ⚙️ M7 — GOAL (инвариант #8: цель (Q(\Phi)\ge 0))

- **Q3-инвариант:** цель: **(Q(\Phi)\ge0)** (не “≥1.125”).
- **Линза:** “интенсивность не отрицательна”: система физически согласована, нет “антисвета”.
- **ML-роль:** глобальная **stability margin**: rollout должен быть устойчивым, без переключения режима.
- **Проверка:** метрика “фокус держится”: рост ошибки на горизонтах (10…1000) остаётся контролируемым.

**Техтермины:**

- **stability margin** — запас устойчивости (сколько система выдерживает).

------

# 2) Жёсткий контракт: кто за что отвечает

Вот твой пункт (2) — фиксируем ответственность так, чтобы потом не спорить:

- **M0 SIGN** — полярность коррекции (фон минус структура).
- **M1 NORM** — шкала/единицы/калибровка координаты.
- **M2 TORUS** — инвариант “только расстояния важны”, переносимость по сдвигам.
- **M3 SYMBOL** — форма ядра/PSF линзы (глобальный отпечаток).
- **M4 FLOOR** — нижняя граница + честная неопределённость.
- **M5 TOEPLITZ** — устойчивость к дискретизации/seq_len.
- **M6 PRIME‑CAP** — ограничение “мощности” глобального вклада.
- **M7 GOAL** — общий запас устойчивости (rollout‑устойчивость).

------

# 3) Как добавить Memory Bank в baseline (clean‑200M)

### Архитектурно (минимальный инвазив)

- Добавляешь **8 learnable memory tokens** размером **n_embd**.
- Конкатенируешь их перед последовательностью:
  **[M0…M7] + [s₁…s_T]**
- Прогоняешь через **Transformer** как обычно.
- Для **MDN** берёшь выходы только для настоящих токенов (без memory).

**Техтермины:**

- **learnable** — обучаемый параметр.
- **token** — элемент последовательности, который участвует в attention.

### Мини‑регуляризации (чтобы memory не превратился в кашу)

1. **slot diversity loss** — делаем слоты разными
   - идея: penalize, если слоты слишком похожи (почти одинаковые вектора).
2. **memory‑cap** — ограничиваем норму memory
   - идея: memory помогает, но не “бог”.
3. **slot dropout** — иногда вырубаем часть слотов на батче
   - идея: чтобы слоты не “учились жить только вместе”.

------

# 4) Проверка: уменьшился ли bias и drift (твои критерии)

### **Bias** = “смещение фокуса”

Проверяем:

- **PIT mean** → должен быть ближе к **0.5**
- signed error one‑step (средняя ошибка со знаком) → ближе к 0

**PIT** (**Probability Integral Transform**) — тест калибровки: если модель честная, PIT равномерный.

### **Drift** = “накопленный дефокус”

Проверяем:

- **Err@h** для h = 10/25/50/100/200/1000
- отдельно **mean‑rollout** и **sample‑rollout** (sampling обычно устойчивее, если модель реально вероятностная)

------

# 5) Training protocol (чтобы не разнести всё)

## Stage A — “подключаем память без хаоса”

- берёшь baseline clean‑200M (твой текущий run)
- добавляешь **Memory Bank**
- **первые 1000–2000 шагов**:
  - LR для memory **в 5–10 раз меньше**, чем для остального (чтобы не улетел фокус)
  - включаешь **slot diversity** + **memory‑cap**

## Stage B — “даём памяти реально работать”

- размораживаешь всё (обычный LR)
- добавляешь **slot dropout** 0.1–0.2
- следишь: PIT mean, Err@100/200

## Stage C — “BIG GOAL: rollout-aware”

- добавляешь **multi-step penalty** на 10–20 шагов (лёгкий, не 1000 сразу)
- или **scheduled sampling** (в конце 20–30%)

Это именно то место, где Q3‑логика “устойчивости линзы” превращается в training.

------

# 6) Супер-конкретно: критерий успеха (без самообмана)

Считаем, что Memory Map Q3 “заработал”, если на одинаковом eval:

- **|PIT mean − 0.5|** падает хотя бы на **20–30%**
- **Err@100** падает хотя бы на **10–20%**
- и главное: улучшение держится и на **tail-val** (высокие γ)

------

## Мини‑код/плейсхолдеры (как подставлять параметры)

Ты подставляешь значения как в шаблоне:
`repo:{username/repo_name} import` → так же и тут: `--data-dir {DATA_DIR} --out-dir {OUT_DIR}`.
То есть **{DATA_DIR}** заменяешь на `data/continuous_clean`, **{OUT_DIR}** на `out/mdn_clean_memq3`, и т.д.

------

Если хочешь, следующий шаг я сделаю максимально “в железо”:

- дам точные флаги CLI (**--n-mem 8**, **--slot-dropout**, **--mem-lr-mult**, **--diversity-w**, **--mem-cap-w**)
- и псевдо‑патч, куда именно в твоём `model/mdn.py` / `model/gpt.py` вставить memory tokens.

Скажи одно: ты хочешь начать с **8 слотов сразу** (как выше), или сделать **быстрый A/B на 4 слотах** (M1/M4/M5/M7) чтобы за день доказать пользу?



> u nas roli teper oboznazenq ? Kakoj механизм заставляет “вот этот конкретный слот = вот этот конкретный инвариант”.? Kak my eto realizowali? ili poka ne realizowali, a poka проверяем, что память как 
> класс снижает bias/drift? 

● Честный ответ: пока НЕ реализовали.

---
  Что сейчас есть:

  M0, M1, M2, M3, M4, M5, M6, M7
    ↓
  8 learnable vectors (безымянные)
    ↓
  diversity_loss заставляет их быть РАЗНЫМИ
    ↓
  но НЕ заставляет "M4 = FLOOR" конкретно

  Slots permutation-symmetric: модель может переименовать их как угодно внутри себя.

---
  Что это значит для эксперимента:

| Stage   | Что проверяем                                | Статус   |
| ------- | -------------------------------------------- | -------- |
| Stage 0 | Memory как класс снижает bias/drift          | ← мы тут |
| Stage 1 | Роли можно идентифицировать post-hoc (probe) | pending  |
| Stage 2 | Роли можно enforced (constraints)            | pending  |

---
  Stage 0 — текущий эксперимент

  Гипотеза: 8 глобальных слотов + regularization → модель сама "распределит" инварианты → bias/drift уменьшатся.

  Если Stage 0 провалится → не имеет смысла усложнять.
  Если Stage 0 сработает → переходим к Stage 1.

---
  Stage 1 — как бы выглядело "привязка роли" (НЕ реализовано)

  Вариант A: Post-hoc probe (анализ после обучения)

  После training смотрим:
  - Какой слот коррелирует с "нормировкой" (M1)?
  - Какой слот активируется при "высоком uncertainty" (M4)?

  # Pseudo: probe each slot's behavior
  for slot_idx in range(8):
      response = measure_slot_response(model, slot_idx, test_scenarios)
      # test_scenarios: "what happens if spacing variance is high?"
      # → slot that responds most = candidate for M4 FLOOR

  Вариант B: Role-specific loss (enforced constraints)

  Добавляем auxiliary losses:

  # M1 (NORM) should be stable across scale changes
  loss_m1 = stability_under_rescaling(slot[1])

  # M4 (FLOOR) should correlate with uncertainty
  loss_m4 = -correlation(slot[4], predicted_sigma.mean())

  # M7 (GOAL) should correlate with rollout error
  loss_m7 = -correlation(slot[7], rollout_stability)

  Но это сложно и требует сначала доказать, что memory вообще помогает.

---
  Итог

  Сейчас запускаем Stage 0:
  - 8 безымянных слотов
  - diversity + cap + dropout
  - Проверяем: bias/drift ↓ ?

  Если да → Stage 1 (probe/enforce roles).
  Если нет → пересмотр гипотезы.