# Speed Optimization Guide for Ampere+ GPUs

> **–¶–µ–ª—å:** –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –ë–ï–ó –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ A40/A100/L40S/H100/RTX 30xx+

---

## TL;DR ‚Äî –ö–æ–ø–∏–ø–∞—Å—Ç–∏ —ç—Ç–æ –≤ training script

```python
import torch

# === CUDA OPTIMIZATIONS (Ampere+) ===
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True
torch.set_float32_matmul_precision('high')  # TF32 by default

# === DATA ON GPU (–≥–ª–∞–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è!) ===
train_data = torch.load('train.pt').to(device)  # –æ–¥–∏–Ω —Ä–∞–∑ –≤ –Ω–∞—á–∞–ª–µ!

# –í training loop:
idx = torch.randint(0, train_data.shape[0], (batch_size,), device=device)
x = train_data[idx]  # GPU‚ÜíGPU = –º–≥–Ω–æ–≤–µ–Ω–Ω–æ

# === MIXED PRECISION (BF16 –Ω–∞ Ampere+) ===
with torch.amp.autocast('cuda', dtype=torch.bfloat16):
    output = model(x)
    loss = criterion(output, target)

# === FUSED OPTIMIZER ===
optimizer = torch.optim.AdamW(params, lr=lr, fused=True)

# === OPTIONAL: torch.compile ===
model = torch.compile(model, mode='reduce-overhead')  # 10-30% speedup
```

---

## –ü–æ–ª–Ω—ã–π Checklist

### 1. Data Loading (üî• –ö–†–ò–¢–ò–ß–ù–û ‚Äî 10-50x speedup!)

| –ú–µ—Ç–æ–¥ | –°–∫–æ—Ä–æ—Å—Ç—å | Trade-off |
|-------|----------|-----------|
| DataLoader (CPU‚ÜíGPU) | üêå | –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö |
| **GPU-only indexing** | üöÄ | –î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –≤–ª–µ–∑–∞—Ç—å –≤ VRAM |

```python
# ‚ùå –ú–ï–î–õ–ï–ù–ù–û ‚Äî –∫–∞–∂–¥—ã–π –±–∞—Ç—á –∫–æ–ø–∏—Ä—É–µ—Ç—Å—è —Å CPU
for batch in DataLoader(dataset, batch_size=256):
    batch = batch.to(device)  # ~10-50ms –Ω–∞ –±–∞—Ç—á!

# ‚úÖ –ë–´–°–¢–†–û ‚Äî –¥–∞–Ω–Ω—ã–µ —É–∂–µ –Ω–∞ GPU
train_data = train_data.to(device)  # –æ–¥–∏–Ω —Ä–∞–∑
idx = torch.randint(0, N, (batch_size,), device=device)
x = train_data[idx]  # ~0.01ms
```

**Trade-off:** –î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –≤–ª–µ–∑–∞—Ç—å –≤ GPU RAM.
- 687MB –¥–∞–Ω–Ω—ã—Ö = OK –¥–ª—è –ª—é–±–æ–π GPU
- 10GB –¥–∞–Ω–Ω—ã—Ö = –Ω—É–∂–Ω–∞ A100 80GB –∏–ª–∏ CPU fallback

---

### 2. Mixed Precision (3x speedup –Ω–∞ matmul)

| –¢–∏–ø | –¢–æ—á–Ω–æ—Å—Ç—å | –°–∫–æ—Ä–æ—Å—Ç—å | –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å |
|-----|----------|----------|---------------|
| FP32 | –í—ã—Å–æ–∫–∞—è | 1x | –í—Å–µ GPU |
| FP16 + GradScaler | –í—ã—Å–æ–∫–∞—è | 8-16x | –í—Å–µ GPU |
| **BF16** | –°—Ä–µ–¥–Ω—è—è | 8-16x | **Ampere+ only** |
| TF32 | –°—Ä–µ–¥–Ω—è—è | 2-3x | Ampere+ only |

```python
# ‚úÖ –î–ª—è Ampere+ (A40, A100, L40S, H100, RTX 30xx+)
with torch.amp.autocast('cuda', dtype=torch.bfloat16):
    output = model(x)

# BF16 –Ω–µ —Ç—Ä–µ–±—É–µ—Ç GradScaler!
# –°—Ä–∞–∑—É backward() –±–µ–∑ scaling
```

**–ü–æ—á–µ–º—É BF16 –ª—É—á—à–µ FP16:**
- BF16: 8-bit exponent = —Ç–æ—Ç –∂–µ range –∫–∞–∫ FP32
- FP16: 5-bit exponent = overflow —Ä–∏—Å–∫ ‚Üí –Ω—É–∂–µ–Ω GradScaler
- –ù–∞ Ampere+ –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å

---

### 3. TF32 (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π 2-3x speedup)

```python
# ‚úÖ –í–∫–ª—é—á–∏—Ç—å –¥–ª—è –≤—Å–µ—Ö matmul –æ–ø–µ—Ä–∞—Ü–∏–π
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.set_float32_matmul_precision('high')
```

TF32 = FP32 range (8-bit exp) + FP16 precision (10-bit mantissa)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–ª—è FP32 –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞ Ampere+
- ~0.1% –ø–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏ ‚Äî –Ω–µ–∑–∞–º–µ—Ç–Ω–æ –¥–ª—è ML

---

### 4. cuDNN Benchmark (5-20% speedup)

```python
torch.backends.cudnn.benchmark = True
```

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:** –ò—â–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è conv/attention.

**Trade-off:**
- –ü–µ—Ä–≤—ã–π forward –ú–ï–î–õ–ï–ù–ù–´–ô (–ø–æ–∏—Å–∫ –∞–ª–≥–æ—Ä–∏—Ç–º–∞)
- –í—Å–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –ë–´–°–¢–†–´–ï
- –†–∞–∑–º–µ—Ä—ã input –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –§–ò–ö–°–ò–†–û–í–ê–ù–ù–´–ï!

---

### 5. Fused Optimizer (10-20% speedup)

```python
# ‚úÖ PyTorch 2.0+
optimizer = torch.optim.AdamW(params, lr=lr, fused=True)

# –ò–ª–∏ Apex (–µ—â—ë –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ –Ω—É–∂–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∞):
# from apex.optimizers import FusedAdam
# optimizer = FusedAdam(params, lr=lr)
```

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:** –û–±—ä–µ–¥–∏–Ω—è–µ—Ç 3 kernel launches –≤ 1.

**Trade-off:** –ù–∏–∫–∞–∫–æ–≥–æ! –ü—Ä–æ—Å—Ç–æ –±—ã—Å—Ç—Ä–µ–µ.

---

### 6. torch.compile (10-30% speedup)

```python
# –ë–∞–∑–æ–≤—ã–π —Ä–µ–∂–∏–º (—Å—Ç–∞–±–∏–ª—å–Ω—ã–π)
model = torch.compile(model)

# –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π (–±—ã—Å—Ç—Ä–µ–µ, –Ω–æ –¥–æ–ª—å—à–µ –∫–æ–º–ø–∏–ª—è—Ü–∏—è)
model = torch.compile(model, mode='reduce-overhead')

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π (—Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π, –¥–æ–ª–≥–∞—è –∫–æ–º–ø–∏–ª—è—Ü–∏—è)
model = torch.compile(model, mode='max-autotune')
```

**Trade-off:**
- –î–æ–ª–≥–∞—è –ø–µ—Ä–≤–∞—è –∫–æ–º–ø–∏–ª—è—Ü–∏—è (30-120 —Å–µ–∫)
- –ù–µ –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã
- Debugging —Å–ª–æ–∂–Ω–µ–µ

**–¢—Ä—é–∫ ‚Äî —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–º–ø–∏–ª—è—Ü–∏—è:**
```python
# –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –±–ª–æ–∫, reuse –¥–ª—è –≤—Å–µ—Ö
compiled_block = torch.compile(model.blocks[0])
for i in range(len(model.blocks)):
    model.blocks[i] = compiled_block
# –ö–æ–º–ø–∏–ª—è—Ü–∏—è 7x –±—ã—Å—Ç—Ä–µ–µ!
```

---

### 7. CUDA Graphs (5x speedup –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –±–∞—Ç—á–µ–π!)

```python
# –î–ª—è batch_size < 64 –≥–¥–µ CPU overhead –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç
g = torch.cuda.CUDAGraph()

# Warmup
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    for _ in range(3):
        output = model(static_input)
torch.cuda.current_stream().wait_stream(s)

# Capture
with torch.cuda.graph(g):
    static_output = model(static_input)

# Replay (–æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ!)
g.replay()
```

**Trade-off:**
- Input/output –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ (—Ç–µ –∂–µ —Ç–µ–Ω–∑–æ—Ä—ã)
- –°–ª–æ–∂–Ω–µ–µ debugging
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç –Ω–∞ –º–∞–ª–µ–Ω—å–∫–∏—Ö –±–∞—Ç—á–∞—Ö

---

### 8. Flash Attention (2-4x speedup, –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏)

```python
# PyTorch 2.0+ ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ SDPA
from torch.nn.functional import scaled_dot_product_attention

# –ò–ª–∏ —è–≤–Ω–æ:
with torch.backends.cuda.sdp_kernel(
    enable_flash=True,
    enable_math=False,
    enable_mem_efficient=False
):
    output = scaled_dot_product_attention(q, k, v)
```

**Trade-off:** –¢–æ–ª—å–∫–æ FP16/BF16, —Ç–æ–ª—å–∫–æ Ampere+.

---

## –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

### –ë—ã—Å—Ç—Ä—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã (—Å–∫–æ—Ä–æ—Å—Ç—å > –≤—Å—ë)

```python
batch_size = 32
seq_len = 256
# GPU-only data
# BF16
# TF32
# Fused AdamW
# cudnn.benchmark = True
# –ù–ï torch.compile (–¥–æ–ª–≥–∞—è –∫–æ–º–ø–∏–ª—è—Ü–∏—è)
```

### Production training (–±–∞–ª–∞–Ω—Å)

```python
batch_size = 256-512
seq_len = 256-512
# –í—Å—ë –≤—ã—à–µ +
# torch.compile(mode='reduce-overhead')
```

### –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å (batch < 64)

```python
batch_size = 32
seq_len = 512
# –í—Å—ë –≤—ã—à–µ +
# CUDA Graphs
```

---

## –ë–µ–Ω—á–º–∞—Ä–∫–∏ (–Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ, A40 GPU)

| –ù–∞—Å—Ç—Ä–æ–π–∫–∞ | Steps/sec | –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ |
|-----------|-----------|------------|
| E4 original (DataLoader) | ~0.5 | CPU‚ÜíGPU bottleneck |
| + GPU-only data | ~5.0 | 10x speedup |
| + BF16 + TF32 | ~8.0 | 16x vs original |
| + Fused AdamW | ~9.0 | 18x vs original |
| + torch.compile | ~11.0 | 22x vs original |

---

## Sources

- [PyTorch Mixed Precision Training](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/)
- [HuggingFace GPU Training Guide](https://huggingface.co/docs/transformers/perf_train_gpu_one)
- [RunPod Mixed Precision Guide](https://www.runpod.io/articles/guides/fp16-bf16-fp8-mixed-precision-speed-up-my-model-training)
- [torch.compile Tutorial](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
- [PyTorch Performance Tuning](https://docs.pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [CUDA Graphs in PyTorch](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/)
- [Sebastian Raschka ‚Äî Accelerating PyTorch](https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training)

---

*Last updated: 2026-01-02*
